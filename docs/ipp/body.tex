\section{Motivation Should}\label{motivation-should}

Introduce the topic of research and explain its academic and industrial
context.

\begin{itemize}
\item
  Establish the general subject area.
\item
  Describe the broad foundations of your study -- provide adequate
  background for readers.
\item
  Indicate the general scope of your project.
\end{itemize}

\section{Introduction and Motivation}\label{introduction-and-motivation}

\begin{quote}
Data pipelines are sets of processes that move and transform data from
various sources to a destination where new value can be derived.
\citep[p.~1]{pipelines_pocket}
\end{quote}

Data pipelines are an important part of today's data landscape,
underpinning manual accounting processes to advanced LLMs. Although data
pipelines are generally discussed as a component in the data engineering
of big data systems, when discussing data pipelines we will consider the
following tasks:

\begin{longtable}[]{@{}lll@{}}
\caption{\label{tbl:table1}Data Pipeline Use Cases}\tabularnewline
\toprule\noalign{}
User & Data goal/task & Tech examples \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
User & Data goal/task & Tech examples \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Business user & Analyse & Excel/Powerquery M \\
Data scientist & Analyse & Python/R \\
Data engineer & Prepare & ETL tools, scripting \\
Database admin & Migrate, Backup & ETL tools, scripting \\
\end{longtable}

As shown in tbl.~\ref{tbl:table1} above

Already we see a range of use cases for the data pipeline, sometimes
these use cases overlap. For example, a data scientist may begin working
with data to create a data product and when completed hand it over to a
data engineer for automating. In this example, the data engineer may be
using python scripts to import source data into dataframes using pythong
scripts, while the data engineer may use ETL tooling to define a
pipeline which stores data on disk for later retrieval by the data
product. This highlights one of the challanges with managing data
pipelines: they are oftern built with varying toolsets that include one
or more languages and/or toolsets.

To summarise, despite this rise in data's importance, there is lacking a
general purpose language for describing a data pipeline and its
transformations. This makes it difficult to have a common cross-system
language for describing such pipelines to facilitate the swapping of one
datasource for another. This can lead to a company being stuck with a
vendor who's pipeline platform would be hard to move away from.

Tabular

To fully develop a data pipleline language is a large undertaking. Doing
some quick calculations, let's say there are 10 file formats + 10
databases leaving us with 20 different possible datastores. Just
``converting'' each of these stores to another leaves us with 400 simple
transformations to support and test. This leaves out that
transformations vary and some are so custom that to define them in
simple terms may not be possible. Let's say that 20 different simple
transformations can be defined, this leaves us with 8000 different
possibilies to test.

Some examples of simple transformations are as follows. Here we will
differenciate between contextual and noncontextual transformations
\citep[p.~106]{pipelines_pocket}:

\begin{itemize}
\tightlist
\item
  noncontextual

  \begin{itemize}
  \tightlist
  \item
    copy data
  \item
    convert data
  \item
    parse data
  \item
    string functions
  \item
    create column

    \begin{itemize}
    \tightlist
    \item
      fixed value
    \item
      simple calculation
    \item
      aggregate calculation
    \item
      windowed calculation
    \end{itemize}
  \item
    drop column
  \item
    filter

    \begin{itemize}
    \tightlist
    \item
      remove duplicates
    \end{itemize}
  \item
    self-join
  \item
    obfuscate sensitive data
  \end{itemize}
\item
  contextual

  \begin{itemize}
  \tightlist
  \item
    simple join
  \item
    multi join
  \item
    windowed join
  \item
    filter
  \end{itemize}
\end{itemize}

Given the scope of such an undertaking, the below proposal will narrow
the scope to the early part of the pipeline known as \emph{data
ingestion}. Data ingestion is the part of the pipeline which moves data
from one datastore to another datastore, optionally applying
noncontextual transformations.

Declarative data pipelines are not a novel concept \citep{ingestbase},
however when implemented they are normally offered as part of a big data
platforms that are integrated in a monolithic system (NiFi, ascend.io)
and their realisation occurs in large cloud based infrastructure
focusing on big data applications. This paper proposes the development
of a declarative system for ingesting data that focuses on smaller data
projects and ease of use.

TODO:

\begin{itemize}
\tightlist
\item
  add more refs
\item
  Engage the readers.
\item
  Provide an overview of the sections that will appear in your proposal
  (optional).
\end{itemize}

\subsection{Problem Statement Should}\label{problem-statement-should}

\begin{quote}
The great virtue of a declarative language is that it makes the intent
clear. You're not saying how to do something, you're saying what you
want to achieve. \citep[p.~39]{patterns_eaa}
\end{quote}

\begin{itemize}
\item
  Answer the question: ``What is the gap that needs to be filled?"
  and/or''What is the problem that needs to be solved?"
\item
  State the problem clearly early in a paragraph.
\item
  Limit the variables you address in stating your problem.
\item
  Consider bordering the problem as a question.
\end{itemize}

\subsection{Problem Statement}\label{problem-statement}

The early stages of a data project often involve (insert refs CRISP-DM,
DataONE, Data Engineering) a consolidation of multiple datasets into a
single datastore or project spanning one or more tables. This paper will
use the data engineering term \emph{data ingestion} to refer to this
phase, concretely: the process of copying data from one or more sources
to a single target. Figure \ref{fig:dataflow} below shows and example of
a dataflow with multiple files landing in a single database scema,
although the destination coud also be a python project or another set of
files in a folder.

\includesvg{img/dataflow.svg}

Setting up the data ingestion process often involves many manual steps
of data discovery, cleansing, standardisation and mapping to bring it
into the target datastore. Depending on the project type the logic for
these transformations can take various forms, from manually copy pasting
data into an Excel spreadsheet to coding transformations in a
python/pandas project. This makes communicating the steps taken to in
this step a challange. It can also introduce reproducability issues when
a decision to change a data definition, source or target while keeping
the remainder of the logic consistent.

To summarise, the ingestion phase of a data project presents some
challanges:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Manual logic defined that could otherwise be automated (mapping of
  fields, definition of datatypes)
\item
  System dependant: work done is not easily trasnferred to an
  alternative system (change in database backend, switching from
  in-memory analysis to datastore)
\item
  Lineage: steps taken to transfer data not easily descernable from
  differing logic by missing manual steps, decoding logic used in
  different systems
\end{enumerate}

\subsection{Research Hypothesis and Objectives
Should}\label{research-hypothesis-and-objectives-should}

Identify the overall aims of the project and the individual measurable
objectives against which you would wish the outcome of the work to be
assessed. Clearly spell out any research hypothesis you are following.

Include a justification (rationale) for the study. Be clear about what
your study will not address.

\subsection{Research Hypothesis and Objectives (not research
project)}\label{research-hypothesis-and-objectives-not-research-project}

The project proposed below A platform agnostic configuration
system/language that can describe data ingestion can alleviate some of
the problems outlined above.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Minimal configuration: data introspection for detecting tables, fields
  and data types.
\item
  System agnostic: source and target datastores can easily be changed.
\item
  Lineage can be read directly from configration.
\end{enumerate}

What is missing

When available as standalone solutions (meltano), they involve a
complicated setup process and are built upon a deadware technology
(singer). It could also be argued that the methods for extracting and
loading of data are imperative, where the configuration must explictly
state the connectors for extraction and loading of data.

This paper proposes a minimal-configuration solution for data ingestion:
extracting from and loading to common data formats. that will perform
introspection on data sources and destinations in order to

This paper proposes a project that will develop and implement a first
phase of a project that will eventually be the first phase of a system
for creating declarative pipelines. Focusing on the ingestion phase and
common formats, the system could be extended to account for
transformation phase.

\subsection{Timeliness and Novelty
Should}\label{timeliness-and-novelty-should}

Explain why the proposed research is of sufficient timeliness and
novelty

\subsection{Timeliness and Novelty}\label{timeliness-and-novelty}

The proposed project is a re-evalulation of data trasnformation from the
ground up. Starting with a base in the early stages of data extraction,
it seeks to lay a common groundwork for a general data transformation
syntax that can be used generally for any data task.

With the current hype of AI and LLMs, these are all based on good data
and often large amounts thereof. This project could be the basis
for\ldots{}

\subsubsection{Timeliness}\label{timeliness}

In the current context of AI and LLM hype a la ChatGPT, this project is
a back to basics re-think of how data pipelines can be generalized and
communicated across differing platforms. With a good benchmark, AI tools
can be further used to optimize some of the data transformation
processes defined herein.

\subsubsection{Novelty}\label{novelty}

Although the theory of a declarative ETL has been around for a while
(insert some ref), there has not been a general purpose impementation.
The proposal herein seeks to get such a system started by beginning with
a basic framework beginning with some common data formats and databases
(insert some ref). In section x we will propose a roadmap for further
enhancements of this project to eventually scale.

\subsection{Significance Should}\label{significance-should}

The proposal should demonstrate the originality of your intended
research. You should therefore explain why your research is important
(for example, by explaining how your research builds on and adds to the
current state of knowledge in the field or by setting out reasons why it
is timely to research your proposed topic) and providing details of any
immediate applications, including further research that might be done to
build on your findings.

\subsection{Significance}\label{significance}

Much attention is paid to big data projects that feed into LLMs and AI
models. It can be said that big data projects can be more and more fit
into single node systems such as laptops, etc. It is for this reason
that \ldots{}

\subsection{Feasibility should (be
obvious?)}\label{feasibility-should-be-obvious}

Comment on the feasibility of the research plans given its limited time
frame and resources. Outline your plans for a feasibility study before
starting e.g.~major implementation work.

\subsection{Feasibility}\label{feasibility}

Given the amount of data formats, databases and data stores available,
adding to that the number of types of transformations. The first phase
of this project must be minimaly scoped to demonstrate its usefulness as
a concept.

\subsection{Beneficiaries should}\label{beneficiaries-should}

Describe how the research will benefit other researchers in the field
and in related disciplines. What will be done to ensure that they can
benefit?

\subsection{Beneficiaries}\label{beneficiaries}

This is more a practical project than a research project. However the
tooling itself could be a benefit to researches if adopted as a means of
defining the data used as part of the research process.

Otherwise the main beneficiaries should be the following:

\begin{itemize}
\tightlist
\item
  Data engineers who work on small to medium data pipelines can use this
  system to define their data ingestion or EtL (Extract,
  small-t/non-contexual transform, Load).
\item
  Data scientists who want a common language to define their data
  sources outside of the code-base
\item
  Data professionals who wish to migrate data from one format/database
  to another, with minimal effort
\end{itemize}

\section{Background and Related Work
should}\label{background-and-related-work-should}

Demonstrate a knowledge and understanding of past and current work in
the subject area, including relevant references like this
\citep{template}.

\section{Background and Related Work}\label{background-and-related-work}

\section{Programme and ?Methodology?
Method}\label{programme-and-methodology-method}

\begin{itemize}
\item
  Detail the methodology to be used in pursuit of the research and
  justify this choice.
\item
  Describe your contributions and novelty and where you will go beyond
  the state-of-the-art (new methods, new tools, new data, new insights,
  new proofs,...)
\item
  Describe the programme of work, indicating the research to be
  undertaken and the milestones that can be used to measure its
  progress.
\item
  Where suitable define work packages and define the dependences between
  these work packages. WPs and their dependences should be shown in the
  Gantt chart in the research plan.
\item
  Explain how the project will be managed.
\item
  State the limitations of your research.
\end{itemize}

\subsection{Risk Assessment}\label{risk-assessment}

\subsection{Ethics}\label{ethics}

\section{Evaluation}\label{evaluation}

\begin{itemize}
\item
  Describe the specific methods of data collection.
\item
  Explain how you intent to analyse and interpret the results.
\end{itemize}

\section{Expected Outcomes}\label{expected-outcomes}

Conclude your research proposal by addressing your predicted outcomes.
What are you hoping to prove/disprove? Indicate how you envisage your
research will contribute to debates and discussions in your particular
subject area:

\begin{itemize}
\item
  How will your research make an original contribution to knowledge?
\item
  How might it fill gaps in existing work?
\item
  How might it extend understanding of particular topics?
\end{itemize}

\section{Research Plan, Milestones and
Deliverables}\label{research-plan-milestones-and-deliverables}

\includesvg{img/gantt.svg}

\begin{itemize}
\tightlist
\item
  revision / iteration
\end{itemize}

\begin{longtable}[]{@{}lll@{}}
\caption{Milestones defined in this project.}\tabularnewline
\toprule\noalign{}
Milestone & Week & Description \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Milestone & Week & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
M1 & 2 & Feasibility study completed \\
M2 & 5 & First prototype implementation completed \\
M3 & 7 & Evaluation completed \\
M4 & 10 & Submission of dissertation \\
\end{longtable}

\begin{longtable}[]{@{}lll@{}}
\caption{List of deliverables defined in this project.}\tabularnewline
\toprule\noalign{}
Deliverable & Week & Description \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Deliverable & Week & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
D1 & 6 & Software tool for . . . \\
D2 & 8 & Evaluation report on . . . \\
D3 & 10 & Dissertation \\
\end{longtable}

\section{Not in Template}\label{not-in-template}

These section are not in the template

\subsection{Proposal}\label{proposal}

The project proposed below A platform agnostic configuration
system/language that can describe data ingestion can alleviate some of
the problems outlined above.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Minimal configuration: data introspection for detecting tables, fields
  and data types.
\item
  System agnostic: source and target datastores can easily be changed.
\item
  Lineage can be read directly from configration.
\end{enumerate}

What is missing

When available as standalone solutions (meltano), they involve a
complicated setup process and are built upon a deadware technology
(singer). It could also be argued that the methods for extracting and
loading of data are imperative, where the configuration must explictly
state the connectors for extraction and loading of data.

This paper proposes a minimal-configuration solution for data ingestion:
extracting from and loading to common data formats. that will perform
introspection on data sources and destinations in order to

This paper proposes a project that will develop and implement a first
phase of a project that will eventually be the first phase of a system
for creating declarative pipelines. Focusing on the ingestion phase and
common formats, the system could be extended to account for
transformation phase.

\subsection{Project}\label{project}

This project has three main modules that will be developed in concert:
eel-project, a folder-based project specification with eel-yaml, a
pipeline configuration language, and eel-cli, a command line tool which
interprets and executes eel-project. Figure \ref{fig:sequence} below is
a high-level sequence diagram as to how these modules interact with a
data pipeline.

\includesvg{img/sequence.svg}

\subsection{Principal Goals}\label{principal-goals}

This scope of this project will focus on the usability aspect of the
language and tool:

\begin{itemize}
\item
  Design a declarative language (YAML) and accompanying project
  structure for extracting and loading data to/from standard file
  formats or databases, including in-process memory structures as
  targets (i.e., pandas data frames).
\item
  Design CLI system to interpret language and optimally orchestrate
  extract/loading procedures, allowing for configuration trees (for
  inheriting config from parent branches) and inferred configuration
  through data introspection (i.e., type inference, reading existing
  meta-data from sources).
\item
  Allow non-contextual transformations to be defined (i.e.~column which
  defines source, index column)
\end{itemize}

\subsection{Secondary Goals}\label{secondary-goals}

These will not be built into the initial project, but design
considerations will be taken in order to allow for the implementation of
the following features.

\begin{itemize}
\item
  Column-level data providence should be built into the system.
\item
  Ability to optimize extract-load procedures based on available tools
  in the executing system, for example utilizing database-native
  extract-load procedures when using a particular database as a
  source/target.
\item
  Multi-thread and/or multi-process when speed/performance gains likely.
\item
  Contextual transformation support.
\end{itemize}

\subsection{eel-yaml}\label{eel-yaml}

A human-readable declarative configuration language defined in a YAML
schema that can be used in most popular editors (such as Visual Studio
Code) to facilitate the user's ability to create the extract-load
configurations manually. The declarative language can be considered a
configuration which details data targets and sources.

\subsection{eel-project}\label{eel-project}

A folder/directory representing the top level of an eel project, all
subfolders and files are considered part of the project's contents. All
folders and certain file types are handled as follows:

\begin{itemize}
\item
  Subfolders are used to organize a hierarchy of targets and/or sources
  in the pipeline, a special configuration file \_.eel.yml is used to
  set the configuration that will be inherited by all containing files
  and folders.
\item
  Recognized data files such as .csv and .xlsx will be treated as
  implicit source configurations, so a .csv file will be treated as
  first a configuration file which refers to itself as the source file.
\item
  Complimentary eel.yml files can augment implicit source configurations
  with additional information such as encoding of the csv. In for a
  complimentary file to be recognized as such, it should have the exact
  same name as the recognized data file with the additional .eel.yml
  extension. For example: data1.csv is recognized as an implicit source
  while data1.csv.eel.yml explicitly defines the encoding that should be
  used.
\item
  Sole eel.yml files (those which are not paired to a data file) may
  contain data source and/or target configurations.
\end{itemize}

The project will stress DRY principals allow for minimum configurations
using some of the following methods:

\begin{itemize}
\item
  Configuration inheritance via directory/folder structure:
  configurations inherit from parent node (i.e., folder). Useful when a
  target object (i.e.~database or table) is the same for multiple data
  sources.
\item
  Use existing metadata (i.e., from database schema) for data types.
\item
  Type inferencing when no data type information available, for example
  in csv files.
\end{itemize}

Configuration and language are two terms that are used interchangeably
in this document, and both refer broadly to the eel-yaml and eel-project
modules.

\subsection{eel-cli}\label{eel-cli}

A minimum viable product for a command line tool that will interpret
eel-project and perform certain actions:

\begin{itemize}
\item
  Show a preview of how eel-cli interprets the current eel-yaml project:
\item
  config tree
\item
  task flow tree
\item
  eel-yaml inherits
\item
  execute dataflow
\end{itemize}

execute the extract-load procedures and also provide granular
information on the extractions.

\subsection{Roadmap}\label{roadmap}

Although these three parts of the project are considered complimentary
and will be developed in concert, it is expected that they would
eventually split and evolve into separate projects in order to separate
the declarative language from any future interpreters that could be
developed to support it.
