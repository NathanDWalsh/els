\section{Motivation}\label{motivation}

\begin{quote}
Data pipelines are sets of processes that move and transform data from
various sources to a destination where new value can be derived.
\citep[p.~1]{pipelines_pocket}
\end{quote}

Data pipelines are an important part of today's data landscape,
underpinning a vast array of processes: from manually created ad-hoc
reports in the form of spreadsheets to advanced LLMs powering ChatGPT.
Their purpose is to make data available and accessible to data
consumers.

A basic unit of a pipeline moves data from a source to a target while
optionally \emph{transforming} the data in the process.
\emph{Transformations} change the state of the data: some basic examples
include cleaning, removing and summarizing data. A basic pipeline unit
is illustrated in Figure \ref{fig:sequence0} from the perspective of a
\emph{pipeline engine}: the process which executes pipelines.

\includesvg{img/sequence0.svg}

The first stage of a pipeline involves a consolidation of multiple
datasets into a single target, we will use the term \emph{ingestion} to
refer to this stage. Figure \ref{fig:dataflow} is an example of an
ingestion phase of a dataflow demonstrating two source files being
loaded to a single database. Each edge in Figure \ref{fig:dataflow}
represents the sequence in Figure \ref{fig:sequence0}.

\includesvg{img/dataflow.svg}

For the last ten years I have been freelancing as a data consultant,
creating and running several data pipeline projects simultaneously with
different clients. This motivated me to think about how data pipeline
projects can be rapidly implemented, iterated and change traced. During
this process I have settled on using the pandas library in Python to
facilitate the \emph{ingestion} phase. The pandas Python library is
popular with data scientists and at its core stores tabular datasets in
a structure called a DataFrame. For the \emph{ingestion} phase the
Python/pandas layer was the \emph{pipeline engine} illustrated in Figure
\ref{fig:sequence0}. This generally functioned an ad-hoc solution but
required a lot of manual tweaking of the code in order to accommodate
exceptions and edge cases. This inspired me to try to capture the
essence of \emph{data ingestion} in an external configuration that could
be read, interpreted and executed by a Python pipeline engine.

The following sections will explore the idea of a pandas declarative
data layer.

\subsection{Problem Statement}\label{problem-statement}

How can \emph{data ingestion}---the process of moving data from multiple
sources to a single target---be streamlined? Before answering this
question, let's first examine how this is currently done in the context
of a hypothetical python project with a single csv source and sql
target.

Below are the specs in bullet form, followed by a code block for
execution.

\begin{itemize}
\tightlist
\item
  source

  \begin{itemize}
  \tightlist
  \item
    \textbf{file:} customer.csv
  \item
    \textbf{encoding:} iso-8859-1 encoding
  \end{itemize}
\item
  transformations

  \begin{itemize}
  \tightlist
  \item
    \textbf{none}
  \end{itemize}
\item
  target

  \begin{itemize}
  \tightlist
  \item
    \textbf{sql database:} sqlite:///my\_database.db
  \item
    \textbf{sql table:} customer
  \item
    \textbf{if table exists:} replace
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{from}\NormalTok{ sqlalchemy }\ImportTok{import}\NormalTok{ create\_engine}

\CommentTok{\# Create a connection to the database}
\NormalTok{engine }\OperatorTok{=}\NormalTok{ create\_engine(}\StringTok{\textquotesingle{}sqlite:///my\_database.db\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Read the CSV file with Brazilian Portuguese encoding}
\NormalTok{df\_customers }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}customer.csv\textquotesingle{}}\NormalTok{, encoding}\OperatorTok{=}\StringTok{\textquotesingle{}iso{-}8859{-}1\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Write the data from the CSV file to the database}
\NormalTok{df\_customers.to\_sql(}\StringTok{\textquotesingle{}customer\textquotesingle{}}\NormalTok{, engine, if\_exists}\OperatorTok{=}\StringTok{\textquotesingle{}replace\textquotesingle{}}\NormalTok{, index}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This being a straightforward example we can quickly identify the source
and the target for the customer dataset. The configuration for this
\emph{ingestion} is hard-coded in our python script. Now imagine that
the customer data comes in multiple csv files: one for each of Brazil's
five regions.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{customers/}
\KeywordTok{|}\ExtensionTok{{-}{-}}\NormalTok{ north.csv}
\KeywordTok{|}\ExtensionTok{{-}{-}}\NormalTok{ northeast.csv}
\KeywordTok{|}\ExtensionTok{{-}{-}}\NormalTok{ central{-}west.csv}
\KeywordTok{|}\ExtensionTok{{-}{-}}\NormalTok{ southeast.csv}
\KeywordTok{|}\ExtensionTok{{-}{-}}\NormalTok{ south.csv}
\end{Highlighting}
\end{Shaded}

In order to accommodate this change to the way that this data will be
ingested we would either have to explicitly code each of the five
regions, or iterate each file in the customers folder. Either way we
loose visibility into what data is being ingested. Another pain arises
if we decide to add the name of the file as a new column as a
transformation.

Below we have a modified spec in bullet form: only the source files and
transformations have been modified. We see how these minor changes
causes a significant change in the code block that follows.

\begin{itemize}
\tightlist
\item
  source

  \begin{itemize}
  \tightlist
  \item
    \textbf{files:} customers/*.csv (five files)
  \item
    \textbf{encoding:} iso-8859-1
  \end{itemize}
\item
  transformations

  \begin{itemize}
  \tightlist
  \item
    add column containing base name of file (without .csv extension)
  \end{itemize}
\item
  target

  \begin{itemize}
  \tightlist
  \item
    \textbf{sql database:} sqlite:///my\_database.db
  \item
    \textbf{sql table:} customer
  \item
    \textbf{if table exists:} replace
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ os}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{from}\NormalTok{ sqlalchemy }\ImportTok{import}\NormalTok{ create\_engine}

\CommentTok{\# Create a connection to the database}
\NormalTok{engine }\OperatorTok{=}\NormalTok{ create\_engine(}\StringTok{\textquotesingle{}sqlite:///my\_database.db\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Specify the directory you want to import files from}
\NormalTok{directory }\OperatorTok{=} \StringTok{\textquotesingle{}customers\textquotesingle{}}

\CommentTok{\# Use os.listdir to get the list of files}
\NormalTok{files }\OperatorTok{=}\NormalTok{ os.listdir(directory)}

\CommentTok{\# Filter the list to only include CSV files}
\NormalTok{csv\_files }\OperatorTok{=}\NormalTok{ [f }\ControlFlowTok{for}\NormalTok{ f }\KeywordTok{in}\NormalTok{ files }\ControlFlowTok{if}\NormalTok{ f.endswith(}\StringTok{\textquotesingle{}.csv\textquotesingle{}}\NormalTok{)]}

\CommentTok{\# Loop over the CSV files and import each one into the database}
\ControlFlowTok{for} \BuiltInTok{file} \KeywordTok{in}\NormalTok{ csv\_files:}
    \CommentTok{\# Read the CSV file with Brazilian Portuguese encoding}
\NormalTok{    file\_path }\OperatorTok{=}\NormalTok{ os.path.join(directory, }\BuiltInTok{file}\NormalTok{)}
\NormalTok{    df\_customers }\OperatorTok{=}\NormalTok{ pd.read\_csv(file\_path, encoding}\OperatorTok{=}\StringTok{\textquotesingle{}iso{-}8859{-}1\textquotesingle{}}\NormalTok{)}

    \CommentTok{\# Get the file name without the extension}
\NormalTok{    file\_name }\OperatorTok{=}\NormalTok{ os.path.splitext(}\BuiltInTok{file}\NormalTok{)[}\DecValTok{0}\NormalTok{]}

    \CommentTok{\# Add the region column}
\NormalTok{    df\_customers[}\StringTok{\textquotesingle{}region\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ file\_name}

    \CommentTok{\# Write the data from the CSV file to the database}
    \CommentTok{\# For the first file imported, replace existing table}
    \CommentTok{\#  On subsequent runs, append to the existing data}
    \CommentTok{\# Assumes csv have same column names, orders and types}
\NormalTok{    if\_exists }\OperatorTok{=} \StringTok{\textquotesingle{}replace\textquotesingle{}} \ControlFlowTok{if}\NormalTok{ engine.has\_table(}\StringTok{\textquotesingle{}customer\textquotesingle{}}\NormalTok{) }\ControlFlowTok{else} \StringTok{\textquotesingle{}append\textquotesingle{}}

\NormalTok{    df\_customers.to\_sql(}\StringTok{\textquotesingle{}customer\textquotesingle{}}\NormalTok{, engine, if\_exists}\OperatorTok{=}\NormalTok{if\_exists, index}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We see that relatively small changes to how the source is structured
requires a big change to the code importing it. This is just giving a
simple example with a single table of data and a few tweaks. If we
consider a project with hundreds of sources, each with different
settings we begin to see the challenges associated with coding and
maintaining this in a python script.

\subsection{Proposed Solution}\label{proposed-solution}

To address the challenges proposed above, a solution that externalizes
source and target data definitions in a pandas ingestion project is
proposed below. In effect, what is being proposed is a pandas
configuration layer which defines sources, simple transformations, and
target data.

Let's show what this configuration might look like for the example given
above:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ingest\_config.yml}
\FunctionTok{target}\KeywordTok{:}
\AttributeTok{  }\FunctionTok{type}\KeywordTok{:}\AttributeTok{ sqllite}
\AttributeTok{  }\FunctionTok{location}\KeywordTok{:}\AttributeTok{ my\_database.db}
\AttributeTok{  }\FunctionTok{table}\KeywordTok{:}\AttributeTok{ customer}
\AttributeTok{  }\FunctionTok{if\_exists}\KeywordTok{:}\AttributeTok{ replace}
\FunctionTok{source}\KeywordTok{:}
\AttributeTok{  }\FunctionTok{type}\KeywordTok{:}\AttributeTok{ csv}
\AttributeTok{  }\FunctionTok{encoding}\KeywordTok{:}\AttributeTok{ iso{-}8859{-}1}
\AttributeTok{  }\FunctionTok{location}\KeywordTok{:}\AttributeTok{ customers/*.csv}
\FunctionTok{add\_columns}\KeywordTok{:}
\AttributeTok{  }\FunctionTok{region}\KeywordTok{:}\AttributeTok{ \_file\_name\_base}
\end{Highlighting}
\end{Shaded}

To execute the pipeline defined above, a simple shell command could be
called

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{eel} \AttributeTok{{-}execute}\NormalTok{ ingest\_config.yml}
\end{Highlighting}
\end{Shaded}
